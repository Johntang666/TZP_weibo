## 论文方面

1. LLM-DDI: Leveraging Large Language Models  for Drug-Drug Interaction Prediction on  Biomedical Knowledge Graph：主要对这篇文章进行理解，个人对其思想的理解就是，首先用已经训练好的模型形成Embedding向量，然后在BK图（GAT图）扩散再聚合，让节点学习特征；
2. GCN-LLM：Combining GCN Structural Learning with LLM Chemical Knowledge for Enhanced Virtual Screening：主要是参考其中[代码](https://github.com/radiaberreziga/gcn-llm-virtual-screening)的思想，其中利用LLM的代码在[src/encoding/compute_embeddings.py](https://github.com/radiaberreziga/gcn-llm-virtual-screening/blob/main/src/compute_embeddings.py)，使用的模型是seyonec/ChemBERTa-zinc-base-v1；
3. [GSRF-DTI](https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-024-01949-3)：虽然这篇论文中没有使用到LLM，但最后的那个使用**随机森林 (RF) 来预测 DTI**未来可能可以考虑到，甚至在消融实验里面也可以使用到；

## 论文可参考代码

1. [DDI-LLM](https://github.com/sshaghayeghs/DDI-LLM)：仓库代码是论文[Can Large Language Models Understand Molecules?](https://arxiv.org/abs/2402.00024)的工作线，论文的代码在https://github.com/sshaghayeghs/LLaMA-VS-GPT ；

## 代码理解

### Optuna

[Optuna](https://optuna.readthedocs.io/zh-cn/latest/index.html)超参数优化框架：可设置血多超参数，例如学习率、批次大小、隐藏层数量或大小、正则化参数；

一个轻量的**超参数自动优化**框架。它用“**定义-采样-评估-剪枝**”的流程，自动探索超参数空间，帮你找到更优的参数组合。核心特点：

- **TPE 等贝叶斯采样器**：比网格/随机搜索更聪明，能快速聚焦“好区域”。
- **剪枝 (Pruning)**：在训练“看起来不行”的 Trial 上**提前停止**，省时。
- **可视化 & 重要性分析**：看收敛曲线、各参数重要性、参数-得分切片图等。
- **分布式/可复现**：支持 SQLite/MySQL 等后端存储，易于并行与复现。
- **多目标优化**：同时最小化 RMSE、推理时延等。

第一次发现是在学习提升算法XGBoost和LightGBM的时候，调参空间可设置多个参数，在实际案例中记得加上验证集早停显著缩短调参时间并防过拟合；alpha正则化L1系数、lambda正则化L2系数